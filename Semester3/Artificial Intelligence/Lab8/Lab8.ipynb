{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbY590Gjk2Ea",
        "outputId": "910bd412-6c4e-4100-a327-050e22998e57"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install markovify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4GJ8Ts7pzKz",
        "outputId": "f83fee39-2ed8-43c4-bad0-992bb0b7fc3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: markovify in /usr/local/lib/python3.10/dist-packages (0.9.4)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (from markovify) (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgyY82xU2B6B",
        "outputId": "d5cb0a0c-4025-403e-d30f-e00ee0fd1ad8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1cF9qCT4wKa",
        "outputId": "0bb79947-3e6d-4bb9-80b3-d0f29808d7d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Generare în limba română: Implementați un sistem care transformă un text (corpus) într-un lanț Markov și folosiți-l pentru a generare un proverb sau o poezie în limba română (folosiți fișierele proverbRo.txt sau poezieRo.txt)\n",
        " - Varianta 1 – Implementați un lanț Markov cu o singură stare"
      ],
      "metadata": {
        "id": "O_GunreIDMly"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mBYzUVpMCjLV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "def loadText(filename):\n",
        "  with open(filename, 'r', encoding='utf-8') as file:\n",
        "    text = file.read().replace('\\n', ' ')\n",
        "  sentences = re.split(r'\\.\\s*', text.strip('.'))\n",
        "  words = []\n",
        "  for sentence in sentences:\n",
        "    words.extend(sentence.split())\n",
        "  return words\n",
        "\n",
        "def createMarkovChain(words):\n",
        "  chain = defaultdict(list)\n",
        "  for currentWord, nextWord in zip(words, words[1:] + ['.']):\n",
        "    chain[currentWord].append(nextWord)\n",
        "  chain[words[-1]].append('.')\n",
        "  return chain\n",
        "\n",
        "def generateText(chain, start, length, maxAttempts=10):\n",
        "  attempt = 0\n",
        "  while attempt < maxAttempts:\n",
        "    currentWord = start.capitalize()\n",
        "    text = [currentWord]\n",
        "    for _ in range(length - 1):\n",
        "      if currentWord not in chain or not chain[currentWord]:\n",
        "        break\n",
        "      currentWord = random.choice(chain[currentWord])\n",
        "      if currentWord == '.':\n",
        "        if text[-1] != '.':\n",
        "          text.append(currentWord)\n",
        "        break\n",
        "      text.append(currentWord.lower())\n",
        "    if len(text) >= length or text[-1] == '.':\n",
        "      return ' '.join(text).rstrip(' .') + '.'\n",
        "    attempt += 1\n",
        "    start = random.choice(list(chain.keys()))\n",
        "  return \"Cannot generate.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = loadText('proverbe.txt')\n",
        "markovChain = createMarkovChain(words)\n",
        "start = random.choice(words)\n",
        "generatedText = generateText(markovChain, start, 10)\n",
        "print(generatedText)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_sNt-QXOV50",
        "outputId": "c2ba7ca3-85aa-4b50-c575-c20fd1eefcda"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buturuga mica rastoarna carul inaintea batranilor sa se plateste prostia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Varianta 2 – Implementați un lanț Markov cu n-stări"
      ],
      "metadata": {
        "id": "U0aE9JMgWJh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadText2(filename):\n",
        "  with open(filename, 'r', encoding='utf-8') as file:\n",
        "    text = file.read().replace('\\n', ' ')\n",
        "  words = text.replace('.', '').replace('!', '').lower().split()\n",
        "  return words\n",
        "\n",
        "def createMarkovChain2(words, n):\n",
        "  chain = defaultdict(list)\n",
        "  for i in range(len(words) - n):\n",
        "    key = tuple(words[i : i+n])\n",
        "    next = words[i+n]\n",
        "    chain[key].append(next)\n",
        "  for key in chain:\n",
        "    if '.' not in chain[key]:\n",
        "      chain[key].append('.')\n",
        "  return chain\n",
        "\n",
        "def generateText2(chain, start, length):\n",
        "  if not start or start not in chain:\n",
        "    return \"Invalid start.\"\n",
        "\n",
        "  words = list(start)\n",
        "  words[0] = words[0].capitalize()\n",
        "  while len(words) < length:\n",
        "    currentKey = tuple(words[-len(start):])\n",
        "    if currentKey in chain and chain[currentKey]:\n",
        "      nextWord = random.choice(chain[currentKey])\n",
        "      if nextWord == '.':\n",
        "        continue\n",
        "      words.append(nextWord)\n",
        "    else:\n",
        "      start = tuple(random.choice([key for key in chain.keys()]))\n",
        "      words.extend(start)\n",
        "  return ' '.join(words).rstrip(' .') + '.'"
      ],
      "metadata": {
        "id": "xS2d1pTVWKkG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = loadText2('proverbe.txt')\n",
        "markovChain = createMarkovChain2(words, 3)\n",
        "start = tuple(random.choice([key for key in markovChain.keys()]))\n",
        "generatedText = generateText2(markovChain, start, 10)\n",
        "print(generatedText)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdL4nz8VYZQw",
        "outputId": "8999d509-1bd5-4f64-b1c8-7e9fb78b9026"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rai dati cezarului cu ciorba sufla si in iaurt cine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Generare în limba engleză:\n",
        " - a. Folosiți biblioteca markovify pentru a genera o strofă de poezie în limba engleză folosind unul din următoarele corpus-uri:"
      ],
      "metadata": {
        "id": "gnlwOEXqkIpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import markovify\n",
        "\n",
        "dataset = load_dataset(\"biglam/gutenberg-poetry-corpus\")\n",
        "\n",
        "poetryTexts = dataset[\"train\"][\"line\"]\n",
        "\n",
        "corpusText = \"\\n\".join(poetryTexts)\n",
        "\n",
        "textModel = markovify.NewlineText(corpusText)\n",
        "\n",
        "poetry = \"\"\n",
        "while not poetry:\n",
        "  poetry = \"\\n\".join([textModel.make_sentence() for _ in range(4)])\n",
        "\n",
        "print(poetry)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmhzjJudlSxX",
        "outputId": "cce58ed0-ace5-4eb5-9d57-663b6d838f83"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With the winds and the pure wild-cherry in bloom!\n",
            "Far may the stars had disappear'd,\n",
            "Beware, I say, but only dream.\n",
            "island. The rich be scaddit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - b. Calculați emoția textului generat, puteți folosi una din următoarele resurse:\n",
        "\n",
        "  - Natural Language Toolkit (nltk) SentimentIntensityAnalyzer\n",
        "  - TextBlob sentiment"
      ],
      "metadata": {
        "id": "-7K26spN1X6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(poetry)\n",
        "\n",
        "polarity = blob.sentiment.polarity\n",
        "\n",
        "if polarity > 0:\n",
        "    sentiment_category = \"Positive\"\n",
        "elif polarity == 0:\n",
        "    sentiment_category = \"Neutral\"\n",
        "else:\n",
        "    sentiment_category = \"Negative\"\n",
        "\n",
        "print(\"Sentiment Polarity:\", polarity)\n",
        "print(\"Sentiment Category:\", sentiment_category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgZsgl1J1dMP",
        "outputId": "6a4befa6-2431-436e-98ff-2a7235f675f3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Polarity: 0.18571428571428572\n",
            "Sentiment Category: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - c. Pentru a adresa limitările de creativitate în poezia generată înlocuiți aleator cuvinte cu sinonime. Se cere ca sinonimele să fie obținute folosind embedding-uri. (i.e. Cuvântul ales e transformat în forma sa embedded și se alege embedding-ul cel mai apropiat care este convertit la string)"
      ],
      "metadata": {
        "id": "nN23qmVK26XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "doc = nlp(poetry)\n",
        "\n",
        "def find_most_similar(word, topn=5):\n",
        "    queried_token = nlp.vocab[word]\n",
        "    if not queried_token.has_vector:\n",
        "        return word\n",
        "\n",
        "    similarities = []\n",
        "    for token in nlp.vocab:\n",
        "        if token.has_vector and token.is_lower == queried_token.is_lower and token.text != word:\n",
        "            similarity = np.dot(queried_token.vector, token.vector) / (np.linalg.norm(queried_token.vector) * np.linalg.norm(token.vector))\n",
        "            similarities.append((token, similarity))\n",
        "\n",
        "    similarities = sorted(similarities, key=lambda item: -item[1])\n",
        "    return similarities[0][0].text if similarities else word\n",
        "\n",
        "revised_text = []\n",
        "for token in doc:\n",
        "    if token.has_vector and not token.is_stop and not token.is_punct:\n",
        "        similar_word = find_most_similar(token.text)\n",
        "        revised_text.append(similar_word)\n",
        "    else:\n",
        "        revised_text.append(token.text)\n",
        "\n",
        "revised_text = \" \".join(revised_text)\n",
        "\n",
        "print(\"Text original:\", poetry)\n",
        "print()\n",
        "print(\"Text modificat:\", revised_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhPGlf1O24gK",
        "outputId": "a3f09758-6c2c-4b87-d138-5db49cd83f87"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text original: With the winds and the pure wild-cherry in bloom!\n",
            "Far may the stars had disappear'd,\n",
            "Beware, I say, but only dream.\n",
            "island. The rich be scaddit.\n",
            "\n",
            "Text modificat: With the island and the rich bloom - bloom in cherry ! \n",
            " Goin' may the o'clock had disappear'd , \n",
            " C++ , I say , but only somethin . \n",
            " where . The pure be scaddit .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - e. Calculați metrica BLEU (Bilingual Evaluation Understudy Score) pentru poezia aleasă"
      ],
      "metadata": {
        "id": "_7s2uVfO8cJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "reference_tokens = [word_tokenize(poetry)]\n",
        "generated_tokens = word_tokenize(revised_text)\n",
        "\n",
        "bleu_score = sentence_bleu(reference_tokens, generated_tokens)\n",
        "\n",
        "print(\"BLEU score:\", bleu_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONuF9ymn8bea",
        "outputId": "d17d2381-584f-4ee1-de47-992b8383864c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.28846838825512133\n"
          ]
        }
      ]
    }
  ]
}